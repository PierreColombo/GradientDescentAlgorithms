{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Classical gradient descent : What to expect from different Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the time when I train Neural Networks on tensorflow I set the optimizer by default to Adam optimizer. I have already look into formula but I have never implemented any of them. Here is some simple code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a three layer Neural Network and some operations to do our back propagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset definition : we use the make_moon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=5000, random_state=42, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition : 3 layers NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feature = 2\n",
    "n_class = 2\n",
    "n_iter = 10\n",
    "def make_network(n_hidden=100):\n",
    "    # Initialize weights with Standard Normal random variables\n",
    "    model = dict(\n",
    "        W1=np.random.randn(n_feature, n_hidden),\n",
    "        W2=np.random.randn(n_hidden, n_class)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch function to get a random sample of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size):\n",
    "    minibatches = []\n",
    "\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be used for training the Neural Network :\n",
    "\n",
    "- softmax function\n",
    "- backward : start by propagating the error to get the gradient of weight between hidden layer and output layer, then we compute the gradient of hidden layer using the error gradient. Using hidden layer gradient, we then compute the gradient of weight between input and hidden layer.\n",
    "- forward : we do series of dot product from input to hidden layer to output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "\n",
    "def forward(x, model):\n",
    "    # Input to hidden\n",
    "    h = x @ model['W1']\n",
    "    # ReLU non-linearity\n",
    "    h[h < 0] = 0\n",
    "\n",
    "    # Hidden to output\n",
    "    prob = softmax(h @ model['W2'])\n",
    "\n",
    "    return h, prob\n",
    "\n",
    "def backward(model, xs, hs, errs):\n",
    "    \"\"\"xs, hs, errs contain all informations (input, hidden state, error) of all data in the minibatch\"\"\"\n",
    "    # errs is the gradients of output layer for the minibatch\n",
    "    dW2 = hs.T @ errs\n",
    "\n",
    "    # Get gradient of hidden layer\n",
    "    dh = errs @ model['W2'].T\n",
    "    dh[hs <= 0] = 0\n",
    "\n",
    "    dW1 = xs.T @ dh\n",
    "\n",
    "    return dict(W1=dW1, W2=dW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the basic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch_grad(model, X_train, y_train):\n",
    "    xs, hs, errs = [], [], []\n",
    "\n",
    "    for x, cls_idx in zip(X_train, y_train):\n",
    "        h, y_pred = forward(x, model)\n",
    "\n",
    "        # Create probability distribution of true label\n",
    "        y_true = np.zeros(n_class)\n",
    "        y_true[int(cls_idx)] = 1.\n",
    "\n",
    "        # Compute the gradient of output layer\n",
    "        err = y_true - y_pred\n",
    "\n",
    "        # Accumulate the informations of minibatch\n",
    "        # x: input\n",
    "        # h: hidden state\n",
    "        # err: gradient of output layer\n",
    "        xs.append(x)\n",
    "        hs.append(h)\n",
    "        errs.append(err)\n",
    "\n",
    "    # Backprop using the informations we get from the current minibatch\n",
    "    return backward(model, np.array(xs), np.array(hs), np.array(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest algorithm. Instead of computing the gradient with all the training sample we use only a small amout of sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(model, X_train, y_train, minibatch_size):\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            model[layer] += alpha * grad[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Momentum is to gain into speed when we go downhill not to be stuck into a wrong minima (if we gain speed we will be able to pass an other uphill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} & = \\frac{4\\pi}{c}\\vec{\\mathbf{j}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{E}} & = 4 \\pi \\rho \\\\\n",
    "\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t} & = \\vec{\\mathbf{0}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{B}} & = 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def momentum(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Nesterov Momentum add one little different bit to the momentum calculation. Instead of calculating gradient of the current position, it calculates the gradient at the approximated new position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nesterov(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        model_ahead = {k: v + gamma * velocity[k] for k, v in model.items()}\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adagrad uses a different approach : the learning rate is adaptive per-parameter. We accumulate the sum of squared of all of our parametersâ€™ gradient, and use that to normalize the learning rate alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adagrad(model, X_train, y_train, minibatch_size):\n",
    "    cache = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            cache[k] += grad[k]**2\n",
    "            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference compared to Adagrad is how we calculate the cache,  this is to fight the monotically increasing gradient. Here, weâ€™re take gamma portion of past accumulated sum of squared gradient, and take 1 - gamma portion of the current squared gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsprop(model, X_train, y_train, minibatch_size):\n",
    "    cache = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            cache[k] = gamma * cache[k] + (1 - gamma) * (grad[k]**2)\n",
    "            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is RMSprop with momentum. So, Adam tries to combine the best of both world of momentum and adaptive learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(model, X_train, y_train, minibatch_size):\n",
    "    M = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    R = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    beta1 = .9\n",
    "    beta2 = .999\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        t = iter\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            M[k] = beta1 * M[k] + (1. - beta1) * grad[k]\n",
    "            R[k] = beta2 * R[k] + (1. - beta2) * grad[k]**2\n",
    "\n",
    "            m_k_hat = M[k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[k] / (1. - beta2**(t))\n",
    "\n",
    "            model[k] += alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and comparing gradient algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_method(method,model, X_train, y_train, minibatch_size) :\n",
    "    if method == \"adam\" :\n",
    "        return adam(model, X_train, y_train, minibatch_size)\n",
    "    if method == \"rmsprop\" :\n",
    "        return rmsprop(model, X_train, y_train, minibatch_size)\n",
    "    if method == \"adagrad\" :\n",
    "        return adagrad(model, X_train, y_train, minibatch_size)\n",
    "    if method == \"nesterov\" :\n",
    "        return nesterov(model, X_train, y_train, minibatch_size)\n",
    "    if method == \"momentum\" :\n",
    "        return momentum(model, X_train, y_train, minibatch_size)\n",
    "    if method == \"sgd\" :\n",
    "        return sgd(model, X_train, y_train, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(method, alpha) :\n",
    "\n",
    "    # Create placeholder to accumulate prediction accuracy\n",
    "    accs = np.zeros(n_experiment)\n",
    "\n",
    "    for k in range(n_experiment):\n",
    "        # Reset model\n",
    "        model = make_network()\n",
    "\n",
    "        # Train the model\n",
    "        model = gradient_method(method,model, X_train, y_train, minibatch_size)\n",
    "\n",
    "        y_pred = np.zeros_like(y_test)\n",
    "\n",
    "        for i, x in enumerate(X_test):\n",
    "            # Predict the distribution of label\n",
    "            _, prob = forward(x, model)\n",
    "            # Get label by picking the most probable one\n",
    "            y = np.argmax(prob)\n",
    "            y_pred[i] = y\n",
    "\n",
    "        # Compare the predictions with the true labels and take the percentage\n",
    "        accs[k] = (y_pred == y_test).sum() / y_test.size\n",
    "    print('Mean {} accuracy: {}, std: {}'.format(method,accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Alpha value ------ 0.1\n",
      "Mean adam accuracy: 0.8749600000000001, std: 0.007029537680388374\n",
      "Mean rmsprop accuracy: 0.8668000000000001, std: 0.01761181421659905\n",
      "Mean adagrad accuracy: 0.8772, std: 0.003283900120283805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrecolombo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/Users/pierrecolombo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/Users/pierrecolombo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in less\n",
      "  if __name__ == '__main__':\n",
      "/Users/pierrecolombo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nesterov accuracy: 0.4768, std: 0.0\n",
      "Mean momentum accuracy: 0.4768, std: 0.0\n",
      "Mean sgd accuracy: 0.6372, std: 0.19645376046286311\n",
      "------ Alpha value ------ 0.01\n",
      "Mean adam accuracy: 0.8787200000000001, std: 0.001394847661933016\n",
      "Mean rmsprop accuracy: 0.86488, std: 0.024303283728747457\n",
      "Mean adagrad accuracy: 0.8618399999999999, std: 0.04239856601348682\n",
      "Mean nesterov accuracy: 0.87104, std: 0.02121976437192457\n",
      "Mean momentum accuracy: 0.8759999999999998, std: 0.005645529204600758\n",
      "Mean sgd accuracy: 0.8317599999999998, std: 0.05985481100128878\n",
      "------ Alpha value ------ 0.001\n",
      "Mean adam accuracy: 0.8254400000000001, std: 0.07495944503529892\n",
      "Mean rmsprop accuracy: 0.82944, std: 0.08210806537728191\n",
      "Mean adagrad accuracy: 0.50504, std: 0.23532705411830573\n",
      "Mean nesterov accuracy: 0.8734400000000001, std: 0.010611993215225874\n",
      "Mean momentum accuracy: 0.8766399999999999, std: 0.0031556932677305515\n",
      "Mean sgd accuracy: 0.87768, std: 0.0020380382724571105\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "eps = 1e-8  # Smoothing to avoid division by zero\n",
    "minibatch_size = 50\n",
    "n_experiment = 10\n",
    "for alpha in [0.1,0.01,0.001] :\n",
    "    print('------ Alpha value ------',alpha)\n",
    "    for method in [\"adam\",\"rmsprop\",\"adagrad\",\"nesterov\",\"momentum\",\"sgd\"] :\n",
    "        compute_accuracy(method, alpha)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
