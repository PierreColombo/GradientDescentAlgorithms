{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Classical gradient descent : What to expect from different Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the time when I train Neural Networks on tensorflow I set the optimizer by default to Adam optimizer. I have already look into formula but I have never implemented any of them. Here is some simple code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a three layer Neural Network and some operations to do our back propagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset definition : we use the make_moon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=5000, random_state=42, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition : 3 layers NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feature = 2\n",
    "n_class = 2\n",
    "n_iter = 10\n",
    "def make_network(n_hidden=100):\n",
    "    # Initialize weights with Standard Normal random variables\n",
    "    model = dict(\n",
    "        W1=np.random.randn(n_feature, n_hidden),\n",
    "        W2=np.random.randn(n_hidden, n_class)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch function to get a random sample of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size):\n",
    "    minibatches = []\n",
    "\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be used for training the Neural Network :\n",
    "\n",
    "- softmax function\n",
    "- backward : start by propagating the error to get the gradient of weight between hidden layer and output layer, then we compute the gradient of hidden layer using the error gradient. Using hidden layer gradient, we then compute the gradient of weight between input and hidden layer.\n",
    "- forward : we do series of dot product from input to hidden layer to output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "\n",
    "def forward(x, model):\n",
    "    # Input to hidden\n",
    "    h = x @ model['W1']\n",
    "    # ReLU non-linearity\n",
    "    h[h < 0] = 0\n",
    "\n",
    "    # Hidden to output\n",
    "    prob = softmax(h @ model['W2'])\n",
    "\n",
    "    return h, prob\n",
    "\n",
    "def backward(model, xs, hs, errs):\n",
    "    \"\"\"xs, hs, errs contain all informations (input, hidden state, error) of all data in the minibatch\"\"\"\n",
    "    # errs is the gradients of output layer for the minibatch\n",
    "    dW2 = hs.T @ errs\n",
    "\n",
    "    # Get gradient of hidden layer\n",
    "    dh = errs @ model['W2'].T\n",
    "    dh[hs <= 0] = 0\n",
    "\n",
    "    dW1 = xs.T @ dh\n",
    "\n",
    "    return dict(W1=dW1, W2=dW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the basic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch_grad(model, X_train, y_train):\n",
    "    xs, hs, errs = [], [], []\n",
    "\n",
    "    for x, cls_idx in zip(X_train, y_train):\n",
    "        h, y_pred = forward(x, model)\n",
    "\n",
    "        # Create probability distribution of true label\n",
    "        y_true = np.zeros(n_class)\n",
    "        y_true[int(cls_idx)] = 1.\n",
    "\n",
    "        # Compute the gradient of output layer\n",
    "        err = y_true - y_pred\n",
    "\n",
    "        # Accumulate the informations of minibatch\n",
    "        # x: input\n",
    "        # h: hidden state\n",
    "        # err: gradient of output layer\n",
    "        xs.append(x)\n",
    "        hs.append(h)\n",
    "        errs.append(err)\n",
    "\n",
    "    # Backprop using the informations we get from the current minibatch\n",
    "    return backward(model, np.array(xs), np.array(hs), np.array(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest algorithm. Instead of computing the gradient with all the training sample we use only a small amout of sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(model, X_train, y_train, minibatch_size):\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            model[layer] += alpha * grad[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Momentum is to gain into speed when we go downhill not to be stuck into a wrong minima (if we gain speed we will be able to pass an other uphill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def momentum(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Nesterov Momentum add one little different bit to the momentum calculation. Instead of calculating gradient of the current position, it calculates the gradient at the approximated new position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nesterov(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        model_ahead = {k: v + gamma * velocity[k] for k, v in model.items()}\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and comparing gradient algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8436159999999999, std: 0.05419130321370764\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 50\n",
    "n_experiment = 100\n",
    "\n",
    "# Create placeholder to accumulate prediction accuracy\n",
    "accs = np.zeros(n_experiment)\n",
    "\n",
    "for k in range(n_experiment):\n",
    "    # Reset model\n",
    "    model = make_network()\n",
    "\n",
    "    # Train the model\n",
    "    model = sgd(model, X_train, y_train, minibatch_size)\n",
    "\n",
    "    y_pred = np.zeros_like(y_test)\n",
    "\n",
    "    for i, x in enumerate(X_test):\n",
    "        # Predict the distribution of label\n",
    "        _, prob = forward(x, model)\n",
    "        # Get label by picking the most probable one\n",
    "        y = np.argmax(prob)\n",
    "        y_pred[i] = y\n",
    "\n",
    "    # Compare the predictions with the true labels and take the percentage\n",
    "    accs[k] = (y_pred == y_test).sum() / y_test.size\n",
    "\n",
    "print('Mean accuracy: {}, std: {}'.format(accs.mean(), accs.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
